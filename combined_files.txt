File: main.py
============================================================
"""
main.py

A single master script that:
 1) Generates buildings (if missing) => buildings_demo.csv
 2) Determines feeders => feeders.csv
 3) Creates lines => lines_demo.csv
 4) Assigns buildings => building_assignments.csv
 5) Builds final model => single-shot PF => sym_output.json / asym_output.json
 6) Generates time-series loads => time_series_loads.csv
 7) Runs time-series PF => time_series_wide.csv
"""

import os
import csv
import json
import random

# Pre-step modules (for generating or computing data)
from generate_buildings import generate_buildings_table
from determine_num_feeders import determine_feeders
from create_mv_lv_lines import create_mv_lv_lines
from assign_buildings import (
    load_buildings, load_lines,
    assign_buildings_to_lines, write_assignments_csv
)
from generate_time_series_loads import generate_time_series_loads

# Final-step modules
from build_network_model import build_network_model
from ascii_generator import generate_ascii_diagram
from json_generator import generate_json_data
from graph_visualizer import visualize_network
from power_flow_solver import solve_power_flow
from time_series_runner_long import run_time_series_pf_long


def get_building_ids_from_csv(buildings_csv):
    """
    Reads a 'buildings_demo.csv' that has columns like [building_id, lat, lon, ...].
    Returns a list of unique building_ids.
    Adjust if your CSV format differs.
    """
    if not os.path.exists(buildings_csv):
        return []
    ids = []
    with open(buildings_csv, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        # we assume each row has a 'building_id'
        for row in reader:
            if "building_id" in row:
                ids.append(row["building_id"])
    return list(set(ids))


def main():
    # ------------------- 1) Generate or Load Buildings -------------------
    buildings_csv = "buildings_demo.csv"
    if not os.path.exists("buildings_demo.csv"):
        print("[main] Generating building data (rich).")
        generate_buildings_table(num_buildings=12, output_path="buildings_demo.csv")
    else:
        print("[main] Using existing 'buildings_demo.csv'.")

    # ------------------- 2) Determine feeders -> feeders.csv ------------
    feeders_csv = "feeders.csv"
    if not os.path.exists(feeders_csv):
        print("[main] Determining feeders => feeders.csv")
        from determine_num_feeders import determine_feeders
        determine_feeders(
            buildings_csv=buildings_csv,
            feeders_csv=feeders_csv,
            buildings_per_feeder=5,
            placement_mode="random_in_bounding_box",
            lat_buffer=0.01,
            lon_buffer=0.01
        )
    else:
        print(f"[main] Found existing '{feeders_csv}' => using it.")

    # ------------------- 3) Create MV/LV lines -> lines_demo.csv --------
    lines_csv = "lines_demo.csv"
    if not os.path.exists(lines_csv):
        print("[main] Creating MV/LV lines => lines_demo.csv")
        substation_id = "MainSubstation"
        substation_lat = 40.150
        substation_lon = -3.550

        # read feeders from feeders.csv
        import csv
        feeder_list = []
        with open(feeders_csv, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                feeder_list.append({
                    "feeder_id": row["feeder_id"],
                    "lat": float(row["lat"]),
                    "lon": float(row["lon"])
                })

        create_mv_lv_lines(
            substation_id=substation_id,
            substation_lat=substation_lat,
            substation_lon=substation_lon,
            feeder_nodes=feeder_list,
            lv_branches_per_feeder=2,  # arbitrary
            output_format="csv",
            output_path=lines_csv
        )
    else:
        print(f"[main] Found existing '{lines_csv}' => using it.")

    # ------------------- 4) Assign buildings -> building_assignments.csv
    assignments_csv = "building_assignments.csv"
    if not os.path.exists(assignments_csv):
        print("[main] Assigning buildings => building_assignments.csv")
        # define node_locations for substation + feeders from the CSV
        substation_id = "MainSubstation"
        substation_lat = 40.150
        substation_lon = -3.550
        node_locations = { substation_id : (substation_lat, substation_lon) }

        # re-load feeder_list
        feeder_list = []
        with open(feeders_csv, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                feeder_list.append(row)
        for fdr in feeder_list:
            fid = fdr["feeder_id"]
            node_locations[fid] = (float(fdr["lat"]), float(fdr["lon"]))

        # read lines
        lines_data = load_lines(lines_csv)
        # replicate logic from create_mv_lv_lines to set LV branches coords
        for ln in lines_data:
            if ln["voltage_level"] == "LV":
                feeder_id = ln["from_id"]
                lvbranch_id = ln["to_id"]
                if lvbranch_id not in node_locations:
                    parts = lvbranch_id.split("_LVbranch_")
                    if len(parts)==2:
                        try:
                            branch_i = int(parts[1])
                            f_lat, f_lon = node_locations[feeder_id]
                            offset_lat = f_lat + 0.001*branch_i
                            offset_lon = f_lon - 0.001*branch_i
                            node_locations[lvbranch_id] = (offset_lat, offset_lon)
                        except:
                            pass

        # load building data
        buildings_data = load_buildings(buildings_csv)
        assignments_list = assign_buildings_to_lines(
            buildings_data,
            lines_data,
            node_locations=node_locations,
            only_lv=True
        )
        write_assignments_csv(assignments_list, assignments_csv)
    else:
        print(f"[main] Found existing '{assignments_csv}' => using it.")

    # ------------------- 5) Build final model => single PF --------------
    print("[main] Building final model => single snapshot PF.")
    from build_network_model import build_network_model
    final_model = build_network_model(
        buildings_path=buildings_csv,
        lines_path=lines_csv,
        assignments_path=assignments_csv
    )
    # ASCII diagram
    from ascii_generator import generate_ascii_diagram
    ascii_str = generate_ascii_diagram(final_model)
    print("\n[main] ASCII Diagram:\n", ascii_str)

    # Convert to JSON => network_model.json
    from json_generator import generate_json_data
    network_json = generate_json_data(final_model)
    with open("network_model.json", "w", encoding="utf-8") as f:
        json.dump(network_json, f, indent=2)
    print("[main] Single-shot PF => 'network_model.json' created.")

    # visualize
    from graph_visualizer import visualize_network
    visualize_network(final_model, show_labels=True, title="Distribution Network Graph")

    # run dummy PF solver => sym_output.json, asym_output.json
    from power_flow_solver import solve_power_flow
    print("[main] Running dummy PF => sym_output.json, asym_output.json.")
    solve_power_flow(
        "network_model.json",
        params_json_path=None,
        sym_out_path="sym_output.json",
        asym_out_path="asym_output.json"
    )

    # ------------------- 6) Generate time-series loads => time_series_loads.csv
    ts_loads_csv = "time_series_loads.csv"
    if not os.path.exists(ts_loads_csv):
        print("[main] Generating time-series loads => time_series_loads.csv.")
        from generate_time_series_loads import generate_time_series_loads
        # get building IDs from buildings_demo.csv
        building_ids = get_building_ids_from_csv(buildings_csv)
        if not building_ids:
            print("[main] No building IDs found in building CSV. Skipping time-series loads.")
        else:
            generate_time_series_loads(
                building_ids=building_ids,
                categories=["heating","facility","generation","storage","total_electricity"],
                start_time="2025-01-01 00:00:00",
                end_time="2025-01-01 06:00:00",
                step_minutes=15,
                output_csv=ts_loads_csv
            )
    else:
        print(f"[main] Found existing '{ts_loads_csv}', using it.")

    # ------------------- 7) Time-series PF => time_series_wide.csv
    if os.path.exists("time_series_loads.csv"):
        run_time_series_pf_long(
            buildings_file="buildings_demo.csv",
            lines_file="lines_demo.csv",
            assignments_file="building_assignments.csv",
            ts_file="time_series_loads.csv",
            output_csv="time_series_long.csv"
        )
        print("[main] Time-series results => time_series_long.csv (LONG format).")
    else:
        print("[main] No time_series_loads.csv => skipping time-series PF.")

    print("[main] Full pipeline complete. Check outputs:\n",
          " - buildings_demo.csv\n",
          " - feeders.csv\n",
          " - lines_demo.csv\n",
          " - building_assignments.csv\n",
          " - network_model.json, sym_output.json, asym_output.json\n",
          " - time_series_loads.csv\n",
          " - time_series_wide.csv\n")


if __name__ == "__main__":
    main()

------------------------------------------------------------

File: time_series_runner_long.py
============================================================
"""
time_series_runner_long.py

Runs time-series power flow for each time step in 'time_series_loads.csv',
then outputs a LONG format CSV with one row per (time_step, entity).

Columns:
  time_step,
  entity_id, record_type, line_id,
  voltage_pu, p_injection_kW, q_injection_kvar, pf,
  i_from_a, i_to_a, line_rating_a, loading_percent

We detect building vs station vs feeder from the node_name:
  - starts with 'B' => building
  - 'MainSubstation' => station
  - starts with 'Feeder' => feeder
  - else => other_node
We approximate Q and PF because the dummy solver doesn't provide them.
"""

import csv
import copy
import math
import os

from build_network_model import build_network_model, update_building_loads
from json_generator import generate_json_data
from power_flow_solver import solve_power_flow_in_memory

def load_time_series_data(ts_file):
    """
    Reads time_series_loads.csv (generated by generate_time_series_loads),
    only parse rows where Energy=='total_electricity' for net building load.
    Returns (load_data, time_headers) where:
      load_data[b_id] = [ val_t0, val_t1, ... ]
      time_headers = [ '00:00:00', '00:15:00', ... ]
    """
    load_data = {}
    time_headers = []

    with open(ts_file, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        headers = reader.fieldnames
        time_headers = headers[2:]  # from third column onward

        for row in reader:
            if row["Energy"].strip().lower() == "total_electricity":
                b_id = row["building_id"]
                values = []
                for h in time_headers:
                    values.append(float(row[h]))
                load_data[b_id] = values

    return load_data, time_headers

def get_node_record_type(node_name):
    """
    Decide if node is building, station, feeder, or other_node.
    """
    if node_name.startswith("B"):
        return "building"
    elif node_name == "MainSubstation":
        return "station"
    elif node_name.startswith("Feeder"):
        return "feeder"
    else:
        return "other_node"

def run_time_series_pf_long(
    buildings_file="buildings_demo.csv",
    lines_file="lines_demo.csv",
    assignments_file="building_assignments.csv",
    ts_file="time_series_loads.csv",
    output_csv="time_series_long.csv"
):
    """
    1) Build a base model from (buildings_file, lines_file, assignments_file).
    2) Load time-series loads from (ts_file) => net building load at each time step.
    3) For each time step:
       a) update building loads in the model
       b) solve PF in memory
       c) parse results for each node + line
       d) write a row for each entity to results

    The final CSV is a LONG format with one row per (time_step, entity).
    Columns:
      time_step, entity_id, record_type, line_id,
      voltage_pu, p_injection_kW, q_injection_kvar, pf,
      i_from_a, i_to_a, line_rating_a, loading_percent
    """
    # 1) Build base model
    print("[time_series_runner_long] Building base model from CSVs.")
    base_model = build_network_model(buildings_file, lines_file, assignments_file)

    # 2) Load time-series net loads
    if not os.path.exists(ts_file):
        print(f"[time_series_runner_long] No {ts_file} => skip.")
        return
    load_data, time_headers = load_time_series_data(ts_file)
    num_steps = len(time_headers)
    print(f"[time_series_runner_long] Found {num_steps} time columns => {time_headers}")

    # We'll keep line ratings, node names for reference
    line_rating_map = {}
    line_id_map = {}
    for ln in base_model["lines"]:
        line_rating_map[ln["id"]] = ln["i_n"]  # nominal rating in A
        line_id_map[ln["id"]] = ln["name"]

    node_id_to_name = {}
    for nd in base_model["nodes"]:
        node_id_to_name[nd["id"]] = nd["name"]

    # We'll gather results in memory as a list of dict rows
    # Each row => {time_step, entity_id, record_type, line_id, voltage_pu, ...}
    results_rows = []

    # 3) Loop over each time step
    for t_idx in range(num_steps):
        time_label = time_headers[t_idx]  # e.g. "00:00:00"
        # copy model
        model_t = copy.deepcopy(base_model)
        # build load dict for this time
        step_load = {}
        for b_id, arr in load_data.items():
            step_load[b_id] = arr[t_idx]
        update_building_loads(model_t, step_load)

        # solve PF
        input_dict = generate_json_data(model_t)
        pf_res = solve_power_flow_in_memory(input_dict)
        sym_data = pf_res["sym"]["data"]  # node:[], line:[], shunt:[]

        # parse node results
        for nd_obj in sym_data["node"]:
            node_id = nd_obj["id"]
            node_name = node_id_to_name[node_id]
            r_type = get_node_record_type(node_name)  # building, station, feeder, other
            v_pu = nd_obj.get("u_pu", 1.0)
            p_w = nd_obj.get("p", 0.0)
            p_kW = p_w/1000.0
            # approximate q, pf
            q_w = 0.3*p_w
            q_kvar = q_w/1000.0
            s_kW = math.sqrt((p_kW**2)+(q_kvar**2)) if abs(p_kW)>1e-9 or abs(q_kvar)>1e-9 else 1e-9
            pf_val = abs(p_kW/s_kW) if s_kW>1e-9 else 1.0

            row = {
                "time_step": time_label,
                "entity_id": node_name,
                "record_type": r_type,
                "line_id": "",
                "voltage_pu": round(v_pu,3),
                "p_injection_kW": round(p_kW,3),
                "q_injection_kvar": round(q_kvar,3),
                "pf": round(pf_val,3),
                "i_from_a": "",
                "i_to_a": "",
                "line_rating_a": "",
                "loading_percent": ""
            }
            results_rows.append(row)

        # parse line results
        for ln_obj in sym_data["line"]:
            l_id = ln_obj["id"]
            i_from = ln_obj.get("i_from", 0.0)
            rating_a = line_rating_map.get(l_id, 9999)
            load_pct = 0.0
            if rating_a>0:
                load_pct = (i_from/rating_a)*100.0

            row = {
                "time_step": time_label,
                "entity_id": "",
                "record_type": "line",
                "line_id": line_id_map[l_id],
                "voltage_pu": "",
                "p_injection_kW": "",
                "q_injection_kvar": "",
                "pf": "",
                "i_from_a": round(i_from,3),
                "i_to_a": "",
                "line_rating_a": rating_a,
                "loading_percent": round(load_pct,2)
            }
            results_rows.append(row)

    # 4) Write final CSV
    out_cols = [
        "time_step","entity_id","record_type","line_id",
        "voltage_pu","p_injection_kW","q_injection_kvar","pf",
        "i_from_a","i_to_a","line_rating_a","loading_percent"
    ]
    with open(output_csv, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=out_cols)
        writer.writeheader()
        for rw in results_rows:
            writer.writerow(rw)

    print(f"[time_series_runner_long] Wrote {len(results_rows)} rows => '{output_csv}'")


if __name__=="__main__":
    """
    Example usage if run standalone:
    python time_series_runner_long.py
    Make sure buildings_demo.csv, lines_demo.csv, building_assignments.csv, time_series_loads.csv exist.
    This will produce time_series_long.csv (one row per time_step per entity).
    """
    run_time_series_pf_long(
        buildings_file="buildings_demo.csv",
        lines_file="lines_demo.csv",
        assignments_file="building_assignments.csv",
        ts_file="time_series_loads.csv",
        output_csv="time_series_long.csv"
    )

------------------------------------------------------------

File: data_lookup.py
============================================================
"""
data_lookup.py

Stores universal constants and default parameters for the project.
"""

DEFAULT_CONFIG = {
    "hv_voltage": 110000,  # 110 kV
    "mv_voltage": 20000,   # 20 kV
    "lv_voltage": 400,     # 0.4 kV

    "hv_slack_voltage_pu": 1.0,  # Slack bus reference
}

DEFAULT_LINE_PARAMS = {
    # Nominal values for line impedances, etc.
    "mv_r1": 1.0,   # example ohms or pu
    "mv_x1": 5.0,   # example ohms or pu
    "lv_r1": 0.5,   # example
    "lv_x1": 1.0,   # example
    "i_n": 300      # nominal current rating
}

DEFAULT_TRANSFORMER_PARAMS = {
    "r1": 0.5,
    "x1": 3.0,
    "i_n": 200,
    "rating_MVA": 1.0
}

# You can add more if needed, e.g. default building load range, etc.

------------------------------------------------------------

File: generate_buildings.py
============================================================
"""
generate_buildings.py

Creates a 'buildings_demo.csv' with richer data:
 - building_id
 - building_function
 - building_type
 - year_range
 - infiltration rates, occupant density, etc.
 - lat, lon
 - peak_load_kW
 - has_solar, solar_capacity_kWp
 - has_battery, battery_capacity_kWh, battery_power_kW

Usage:
  python generate_buildings.py
  -> generates a default 'buildings_demo.csv' with random data.
"""

import csv
import random

def weighted_choice(choices):
    """
    Given a list of (item, weight) tuples, randomly select an item
    according to the specified probabilities.
    """
    total = sum(weight for item, weight in choices)
    r = random.uniform(0, total)
    upto = 0
    for item, weight in choices:
        if upto + weight >= r:
            return item
        upto += weight
    return choices[-1][0]  # fallback

def generate_rich_building_data(num_buildings=20):
    """
    Generate synthetic building data with infiltration, occupant density,
    building function, building type, etc. Also includes solar/battery info.
    Returns a list of dicts, each representing one building row.
    """

    building_function_choices = [
        ("residential", 0.60),
        ("non_residential", 0.40),
    ]
    residential_type_choices = [
        ("Two-and-a-half-story House", 0.3),
        ("Corner House", 0.3),
        ("Apartment", 0.4),
    ]
    nonres_type_choices = [
        ("Office Function", 0.5),
        ("Meeting Function", 0.3),
        ("Education Function", 0.2),
    ]
    year_range_choices = [
        ("< 1945", 0.1),
        ("1945 - 1964", 0.1),
        ("1965 - 1974", 0.1),
        ("1975 - 1991", 0.2),
        ("1992 - 2005", 0.3),
        ("2006 - 2014", 0.1),
        ("2015 and later", 0.1)
    ]

    def random_infiltration_rate(build_func):
        if build_func == "residential":
            return (round(random.uniform(0.5, 0.8), 2),
                    round(random.uniform(0.8, 1.0), 2))
        else:
            return (round(random.uniform(0.7, 1.0), 2),
                    round(random.uniform(1.0, 1.3), 2))

    def random_occupant_density(build_func):
        # occupant_density_m2_per_person
        if build_func == "residential":
            min_val = random.choice([10, 12, 15])
            max_val = min_val + random.randint(5, 10)
            return (min_val, max_val)
        else:
            min_val = random.choice([5, 8, 10])
            max_val = min_val + random.randint(3, 10)
            return (min_val, max_val)

    def random_dhw_usage(build_func):
        # liters_per_person_per_day
        if build_func == "residential":
            return (random.randint(30, 50), random.randint(50, 70))
        else:
            # Non-res might have minimal usage
            return (random.randint(0, 5), random.randint(5, 15))

    def random_heating_setpoint():
        # e.g. day_min ~ [19..20], day_max = day_min+1
        day_min = round(random.uniform(19.0, 20.0), 1)
        day_max = round(day_min + 1.0, 1)
        return (day_min, day_max)

    def random_cooling_setpoint():
        # e.g. day_min ~ [23.5..24.5], day_max=day_min+1
        day_min = round(random.uniform(23.5, 24.5), 1)
        day_max = round(day_min + 1.0, 1)
        return (day_min, day_max)

    def random_lpd(build_func):
        if build_func == "residential":
            return (round(random.uniform(3,5),1),
                    round(random.uniform(5,7),1))
        else:
            return (round(random.uniform(8,12),1),
                    round(random.uniform(12,16),1))

    buildings = []
    for i in range(num_buildings):
        b_id = f"B{i+1:04d}"  # B0001, B0002,...

        build_func = weighted_choice(building_function_choices)
        if build_func == "residential":
            b_type = weighted_choice(residential_type_choices)
        else:
            b_type = weighted_choice(nonres_type_choices)

        year_range = weighted_choice(year_range_choices)
        inf_min, inf_max = random_infiltration_rate(build_func)
        occd_min, occd_max = random_occupant_density(build_func)
        dhw_min, dhw_max = random_dhw_usage(build_func)
        h_min, h_max = random_heating_setpoint()
        c_min, c_max = random_cooling_setpoint()
        lpd_min, lpd_max = random_lpd(build_func)

        area_val = random.randint(30,300)
        perimeter_val = random.randint(20,100)
        height_val = random.uniform(3.0,12.0)

        # random lat/lon for demonstration
        lat_val = round(random.uniform(40.10, 40.20),5)
        lon_val = round(random.uniform(-3.60, -3.50),5)

        # Optional peak_load_kW
        peak_load = round(random.uniform(5, 200), 1)

        # ------------------ NEW: solar & battery info ------------------
        # Decide if building has solar
        has_solar = (random.random() < 0.4)  # 40% chance
        solar_capacity_kWp = 0.0
        if has_solar:
            solar_capacity_kWp = round(random.uniform(2.0, 15.0), 1)

        # Decide if building has battery
        has_battery = (random.random() < 0.3)  # 30% chance
        battery_capacity_kWh = 0.0
        battery_power_kW = 0.0
        if has_battery:
            battery_capacity_kWh = round(random.uniform(5.0, 30.0), 1)
            battery_power_kW = round(random.uniform(3.0, 10.0), 1)

        row = {
            "building_id": b_id,
            "lat": lat_val,
            "lon": lon_val,
            "peak_load_kW": peak_load,

            "ogc_fid": random.randint(1000000,9999999),
            "pand_id": f"{random.uniform(1e13,2e13):.2E}",
            "label": random.choice(["A","B","C","D"]),
            "gem_hoogte": random.randint(2,8),
            "gem_bouwlagen": random.randint(1,4),
            "b3_dak_type": random.choice(["flat","pitched","multiple horizontal"]),
            "b3_opp_dak_plat": random.randint(20,100),
            "b3_opp_dak_schuin": random.randint(10,60),
            "postcode": f"{random.randint(1000,9999)}{random.choice(['AB','CD','EF','GH','XZ'])}",
            "area": area_val,
            "perimeter": perimeter_val,
            "height": round(height_val,1),
            "bouwjaar": random.randint(1930,2023),
            "age_range": year_range,
            "average_wwr": round(random.uniform(0.1,0.4),2),
            "building_function": build_func,
            "building_type": b_type,

            # infiltration, occupant density, etc.
            "infiltration_rate_min": inf_min,
            "infiltration_rate_max": inf_max,
            "occupant_density_min": occd_min,
            "occupant_density_max": occd_max,
            "dhw_liters_per_person_day_min": dhw_min,
            "dhw_liters_per_person_day_max": dhw_max,
            "heating_day_setpoint_min": h_min,
            "heating_day_setpoint_max": h_max,
            "cooling_day_setpoint_min": c_min,
            "cooling_day_setpoint_max": c_max,
            "lighting_power_density_wm2_min": lpd_min,
            "lighting_power_density_wm2_max": lpd_max,

            "fan_power_w": random.randint(30,200),
            "hrv_efficiency": 0.75,

            # NEW columns for solar/battery
            "has_solar": has_solar,
            "solar_capacity_kWp": solar_capacity_kWp,
            "has_battery": has_battery,
            "battery_capacity_kWh": battery_capacity_kWh,
            "battery_power_kW": battery_power_kW
        }
        buildings.append(row)

    return buildings

def write_buildings_to_csv(buildings, csv_filename="buildings_demo.csv"):
    """
    Writes the generated building data to CSV.
    """
    if not buildings:
        print("[generate_buildings] No buildings to write.")
        return

    fieldnames = list(buildings[0].keys())
    with open(csv_filename, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(buildings)

    print(f"[generate_buildings] Wrote {len(buildings)} buildings => {csv_filename}")

def generate_buildings_table(
    num_buildings=20,
    output_format="csv",
    output_path="buildings_demo.csv",
    lat_range=(40.10,40.20),
    lon_range=(-3.60,-3.50),
    demand_range=(5,60),
    building_type_distribution=None
):
    """
    Replaces or overrides the old function. 
    Now we call generate_rich_building_data(...) 
    to produce a more "rich" building CSV with solar/battery columns.
    """
    buildings = generate_rich_building_data(num_buildings=num_buildings)
    write_buildings_to_csv(buildings, csv_filename=output_path)

if __name__ == "__main__":
    # If run standalone:
    generate_buildings_table(num_buildings=12, output_path="buildings_demo.csv")

------------------------------------------------------------

File: determine_num_feeders.py
============================================================
"""
determine_num_feeders.py

Calculates how many feeders we want and writes a CSV with each feeder's ID and lat/lon.

Steps:
  1. Read buildings CSV to determine number of buildings (or total load).
  2. Decide how many feeders by a simple ratio or load-based approach.
  3. Place feeders at random or user-defined lat/lon within the bounding box of the buildings.
  4. Write feeders.csv with columns: feeder_id, lat, lon
"""

import csv
import math
import random
import os

def get_num_feeders_simple(num_buildings, buildings_per_feeder=50):
    """
    Returns how many feeders if each feeder can handle up to 'buildings_per_feeder'.
    Example: 120 buildings, 50 per feeder -> 3 feeders.
    """
    feeders = math.ceil(num_buildings / buildings_per_feeder)
    return feeders

def get_num_feeders_by_load(total_load_kW, feeder_capacity_kW=2000):
    """
    Returns how many feeders if each feeder can handle up to 'feeder_capacity_kW'.
    Example: total_load=5500 kW, capacity=2000 kW -> 3 feeders
    """
    return math.ceil(total_load_kW / feeder_capacity_kW)

def determine_feeders(
    buildings_csv="buildings.csv",
    feeders_csv="feeders.csv",
    buildings_per_feeder=50,
    placement_mode="random_in_bounding_box",
    lat_buffer=0.01,
    lon_buffer=0.01
):
    """
    1) Reads 'buildings_csv' to find how many buildings (and optionally min/max lat/lon).
    2) Calculates how many feeders are needed (simple ratio).
    3) For each feeder, picks lat/lon either randomly in the bounding box
       or uses a simple pattern. 
    4) Writes 'feeders_csv' with columns: feeder_id, lat, lon

    :param buildings_csv: path to buildings data (must have lat, lon columns)
    :param feeders_csv: output CSV with feeder_id, lat, lon
    :param buildings_per_feeder: ratio for feeder count
    :param placement_mode: "random_in_bounding_box" or "center" or any custom approach
    :param lat_buffer, lon_buffer: extra padding around building bounding box
    """
    # 1) Load building data
    if not os.path.exists(buildings_csv):
        raise FileNotFoundError(f"Cannot find buildings file: {buildings_csv}")

    with open(buildings_csv, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        buildings = list(reader)

    num_buildings = len(buildings)
    if num_buildings < 1:
        raise ValueError("No buildings found in the CSV. Cannot determine feeders.")

    # gather lat/lon min/max
    lat_values = []
    lon_values = []
    total_load = 0.0
    for b in buildings:
        lat_values.append(float(b["lat"]))
        lon_values.append(float(b["lon"]))
        # optionally gather load if you prefer a load-based approach
        if "peak_load_kW" in b:
            total_load += float(b["peak_load_kW"])

    min_lat, max_lat = min(lat_values), max(lat_values)
    min_lon, max_lon = min(lon_values), max(lon_values)

    # 2) Decide feeder count
    feeders_needed = get_num_feeders_simple(num_buildings, buildings_per_feeder)
    # (Or we could do get_num_feeders_by_load(total_load, feeder_capacity_kW=2000))

    # 3) Place feeders
    # We'll store them in a list of dict: { "feeder_id": ..., "lat": ..., "lon": ... }
    feeder_list = []
    for i in range(feeders_needed):
        fid = f"Feeder{i+1}"
        if placement_mode == "random_in_bounding_box":
            # pick random lat/lon in [min_lat - lat_buffer, max_lat + lat_buffer], etc.
            feeder_lat = random.uniform(min_lat - lat_buffer, max_lat + lat_buffer)
            feeder_lon = random.uniform(min_lon - lon_buffer, max_lon + lon_buffer)
        else:
            # fallback: center or other approach
            # e.g., place them evenly spaced across bounding box
            frac = (i+1)/(feeders_needed+1)  # fraction in [1/(N+1), ..., N/(N+1)]
            feeder_lat = (1-frac)*(min_lat - lat_buffer) + frac*(max_lat + lat_buffer)
            feeder_lon = (1-frac)*(min_lon - lon_buffer) + frac*(max_lon + lon_buffer)

        feeder_list.append({
            "feeder_id": fid,
            "lat": round(feeder_lat, 6),
            "lon": round(feeder_lon, 6)
        })

    # 4) Write feeders.csv
    with open(feeders_csv, "w", newline="", encoding="utf-8") as f:
        fieldnames = ["feeder_id", "lat", "lon"]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(feeder_list)

    print(f"[determine_num_feeders] Found {num_buildings} buildings, placed {feeders_needed} feeders.")
    print(f"  bounding box: lat=[{min_lat:.4f}, {max_lat:.4f}], lon=[{min_lon:.4f}, {max_lon:.4f}]")
    print(f"  feeders saved to '{feeders_csv}'")

    return feeder_list


if __name__ == "__main__":
    # Example usage:
    # 1) Suppose we have a buildings_demo.csv with lat/lon columns
    # 2) We want 1 feeder per 5 buildings
    # 3) We'll place feeders randomly in the bounding box
    buildings_file = "buildings_demo.csv"
    feeders_file = "feeders.csv"
    determine_feeders(
        buildings_csv=buildings_file,
        feeders_csv=feeders_file,
        buildings_per_feeder=5,
        placement_mode="random_in_bounding_box",
        lat_buffer=0.01,
        lon_buffer=0.01
    )

------------------------------------------------------------

File: build_network_model.py
============================================================
"""
build_network_model.py

Creates a final 'model' dictionary for your distribution network, using:
  - buildings.csv/json
  - lines.csv/json
  - building_assignments.csv/json
  - node locations (dict or file)

and merges them into one data structure:

model = {
  "nodes": [...],
  "lines": [...],
  "links": [...],
  "loads": [...],
  "sources": [...],
  "shunts": []
}

You can then pass this model to ascii_generator.py, json_generator.py, or a time-series runner, etc.
"""

import csv
import json
import os
from data_lookup import DEFAULT_CONFIG, DEFAULT_LINE_PARAMS, DEFAULT_TRANSFORMER_PARAMS

def load_csv_or_json(filepath):
    """
    A utility to read either CSV or JSON into a list of dicts.
    """
    if filepath.lower().endswith(".csv"):
        with open(filepath, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            return list(reader)
    else:
        # assume JSON
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)

def build_network_model(
    buildings_path="buildings.csv",
    lines_path="lines.csv",
    assignments_path="building_assignments.csv",
    node_locations=None,
    config=None
):
    """
    Builds a unified model dict for subsequent usage.

    :param buildings_path: CSV/JSON file with building data (id, lat, lon, peak_load, etc.)
    :param lines_path: CSV/JSON file with lines data (line_id, from_id, to_id, voltage_level, etc.)
    :param assignments_path: CSV/JSON file with building->line assignments
    :param node_locations: dict { node_id: (lat, lon) }, optional if lines_path doesn't store lat/lon
    :param config: dictionary of default config, from data_lookup.py or custom
    :return: a dictionary with structure:
       {
         "nodes": [],
         "lines": [],
         "links": [],
         "loads": [],
         "sources": [],
         "shunts": []
       }
    """
    if config is None:
        config = DEFAULT_CONFIG

    if node_locations is None:
        node_locations = {}

    # Initialize model
    model = {
        "nodes": [],
        "lines": [],
        "links": [],
        "loads": [],
        "sources": [],
        "shunts": []
    }
    current_id = 1

    def get_new_id():
        nonlocal current_id
        nid = current_id
        current_id += 1
        return nid

    # 1) Load the input files
    buildings_data = load_csv_or_json(buildings_path) if os.path.exists(buildings_path) else []
    lines_data = load_csv_or_json(lines_path) if os.path.exists(lines_path) else []
    assignments_data = load_csv_or_json(assignments_path) if os.path.exists(assignments_path) else []

    # 2) Create nodes from lines_data (unique from_id, to_id)
    node_ids = set()  # track which node "names" we've added

    for ln in lines_data:
        f_id = ln["from_id"]
        t_id = ln["to_id"]

        # from_id node
        if f_id not in node_ids:
            latlon = node_locations.get(f_id, (0, 0))
            node_model_id = get_new_id()
            model["nodes"].append({
                "id": node_model_id,
                "name": f_id,
                "u_rated": config.get("mv_voltage", 20000),
                "lat": latlon[0],
                "lon": latlon[1]
            })
            node_ids.add(f_id)

        # to_id node
        if t_id not in node_ids:
            latlon = node_locations.get(t_id, (0, 0))
            node_model_id = get_new_id()
            model["nodes"].append({
                "id": node_model_id,
                "name": t_id,
                "u_rated": config.get("mv_voltage", 20000),
                "lat": latlon[0],
                "lon": latlon[1]
            })
            node_ids.add(t_id)

    # 2a) Create a helper dict to map node "name" -> "model ID"
    name_to_model_id = {}
    for nd in model["nodes"]:
        name_to_model_id[nd["name"]] = nd["id"]

    # 3) Add lines to model["lines"]
    for ln in lines_data:
        line_id = get_new_id()
        from_node = name_to_model_id[ln["from_id"]]
        to_node = name_to_model_id[ln["to_id"]]
        voltage_level = ln.get("voltage_level", "MV")

        # default R/X based on voltage_level
        if voltage_level == "MV":
            r1 = DEFAULT_LINE_PARAMS["mv_r1"]
            x1 = DEFAULT_LINE_PARAMS["mv_x1"]
        else:
            # "LV"
            r1 = DEFAULT_LINE_PARAMS["lv_r1"]
            x1 = DEFAULT_LINE_PARAMS["lv_x1"]

        model["lines"].append({
            "id": line_id,
            "name": ln["line_id"],
            "from_node": from_node,
            "to_node": to_node,
            "r1": r1,
            "x1": x1,
            "i_n": DEFAULT_LINE_PARAMS["i_n"],
            "voltage_level": voltage_level
        })

    # 4) Create building nodes + loads
    #    p_kW is the default from CSV ("peak_load_kW") but in a time-series scenario,
    #    you can override these values later using 'update_building_loads' if you want.
    building_name_to_id = {}
    for b in buildings_data:
        bname = b["building_id"]
        latB = float(b.get("lat", 0))
        lonB = float(b.get("lon", 0))
        peak_load_kW = float(b.get("peak_load_kW", 0))

        # create node for building
        node_id = get_new_id()
        model["nodes"].append({
            "id": node_id,
            "name": bname,
            "u_rated": config.get("lv_voltage", 400),
            "lat": latB,
            "lon": lonB
        })
        building_name_to_id[bname] = node_id

        # add a load entry
        load_id = get_new_id()
        model["loads"].append({
            "id": load_id,
            "node": node_id,
            "status": 1,
            "type": 1,  # or a type code from config
            "p_kW": peak_load_kW
        })

    # 5) Use building_assignments to create "links"
    #    We'll link building -> line's "to_node" for simplicity.
    line_name_to_nodes = {}
    for ln in model["lines"]:
        line_name = ln["name"]  # e.g. "L0001"
        line_name_to_nodes[line_name] = (ln["from_node"], ln["to_node"])

    for asg in assignments_data:
        bname = asg["building_id"]  # e.g. "B0001"
        line_name = asg["line_id"]  # e.g. "L0001"
        dist_km = asg.get("distance_km", 0)

        if bname not in building_name_to_id:
            continue
        if line_name not in line_name_to_nodes:
            continue

        building_node_id = building_name_to_id[bname]
        from_node_id, to_node_id = line_name_to_nodes[line_name]

        link_id = get_new_id()
        model["links"].append({
            "id": link_id,
            "name": f"Link_{bname}_to_{line_name}",
            "from_node": building_node_id,
            "to_node": to_node_id,
            "dist_km": dist_km
        })

    # 6) Optionally add a "source" for the substation if known
    if "MainSubstation" in name_to_model_id:
        source_id = get_new_id()
        model["sources"].append({
            "id": source_id,
            "node": name_to_model_id["MainSubstation"],
            "status": 1,
            "u_ref": config.get("hv_slack_voltage_pu", 1.0)
        })

    return model


def update_building_loads(model, load_dict):
    """
    Allows overriding building loads in the model in memory.
    :param model: the final model dict (with "loads")
    :param load_dict: { building_name: p_kW_value, ... } or
                      { node_id: p_kW_value, ... }
    For example: { "B0001": 50.0, "B0002": 10.0 }

    If your model uses building_name == node name, you can match them.
    Otherwise, store a mapping from building_name->node ID and adapt.
    """
    # We'll assume building_name == model "node" name for loads
    # You might track them differently if needed.
    name_to_load_id = {}
    # build a quick map from node name -> load entry
    for ld in model["loads"]:
        node_id = ld["node"]
        # find the node in model["nodes"] to get name
        node_obj = next((n for n in model["nodes"] if n["id"] == node_id), None)
        if node_obj:
            b_name = node_obj["name"]
            name_to_load_id[b_name] = ld["id"]

    for bname, load_val in load_dict.items():
        # find load entry
        if bname in name_to_load_id:
            ld_id = name_to_load_id[bname]
            # update that load
            for ld in model["loads"]:
                if ld["id"] == ld_id:
                    ld["p_kW"] = load_val
                    break


if __name__ == "__main__":
    # Example usage
    model = build_network_model("buildings_demo.csv", "lines_demo.csv", "building_assignments.csv")
    print(f"Model has {len(model['nodes'])} nodes, {len(model['loads'])} loads.")

    # Suppose we want to override B0001's load to 45.0 kW
    new_loads = {"B0001": 45.0}
    update_building_loads(model, new_loads)

    # Now B0001's load is changed in memory
    for ld in model["loads"]:
        pass  # could print or verify

------------------------------------------------------------

File: create_mv_lv_lines.py
============================================================
"""
create_mv_lv_lines.py

Creates MV and LV lines for a distribution network, given:
  1) A substation node with lat/lon
  2) A list of feeder nodes, each with lat/lon
  3) (Optionally) we can create a few "LV branches" off each feeder node,
     but not assigned to buildings. This is just a topology skeleton.

Outputs lines in CSV or JSON with fields:
   line_id, from_id, to_id, length_km, voltage_level
"""

import csv
import json
import math

def distance_lat_lon(lat1, lon1, lat2, lon2):
    """
    Approximate distance in km using the haversine formula.
    """
    R = 6371.0
    lat1_r = math.radians(lat1)
    lon1_r = math.radians(lon1)
    lat2_r = math.radians(lat2)
    lon2_r = math.radians(lon2)

    dlat = lat2_r - lat1_r
    dlon = lon2_r - lon1_r

    a = math.sin(dlat / 2)**2 + math.cos(lat1_r) * math.cos(lat2_r) * math.sin(dlon / 2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    return R * c

def create_mv_lv_lines(
    substation_id="Substation",
    substation_lat=40.1000,
    substation_lon=-3.5000,
    feeder_nodes=None,
    lv_branches_per_feeder=0,
    output_format="csv",
    output_path="lines.csv"
):
    """
    :param substation_id: name/id for the substation node
    :param substation_lat, substation_lon: substation location
    :param feeder_nodes: list of dicts:
        [
          {
            "feeder_id": "Feeder1",
            "lat": 40.105,
            "lon": -3.505,
          },
          {
            "feeder_id": "Feeder2",
            "lat": 40.110,
            "lon": -3.520,
          },
          ...
        ]
    :param lv_branches_per_feeder: how many LV lines to create from each feeder node
        (purely to illustrate an LV topology skeleton, not assigned to buildings).
    :param output_format: "csv" or "json"
    :param output_path: file path to write lines
    :return: a list of line dicts with fields
        {
          "line_id": ...,
          "from_id": ...,
          "to_id": ...,
          "length_km": ...,
          "voltage_level": "MV" or "LV"
        }
    """
    if feeder_nodes is None:
        feeder_nodes = []

    lines_list = []
    line_count = 1

    # 1) For each feeder node, create an MV line from the substation to that node
    for feeder in feeder_nodes:
        f_id = feeder["feeder_id"]
        f_lat = feeder["lat"]
        f_lon = feeder["lon"]

        dist_km = distance_lat_lon(substation_lat, substation_lon, f_lat, f_lon)
        line_id = f"L{line_count:04d}"
        lines_list.append({
            "line_id": line_id,
            "from_id": substation_id,
            "to_id": f_id,
            "length_km": round(dist_km, 4),
            "voltage_level": "MV"
        })
        line_count += 1

        # 2) Optionally create some LV lines to represent branches from the feeder node
        for branch_i in range(lv_branches_per_feeder):
            # We'll create a simple pseudo-lv node at a random small offset
            # In reality, you'd define actual coordinates or further branching logic
            offset_lat = f_lat + 0.001 * (branch_i + 1)  # purely demonstration
            offset_lon = f_lon - 0.001 * (branch_i + 1)
            dist_lv = distance_lat_lon(f_lat, f_lon, offset_lat, offset_lon)

            lv_line_id = f"L{line_count:04d}"
            lv_node_id = f"{f_id}_LVbranch_{branch_i+1}"
            lines_list.append({
                "line_id": lv_line_id,
                "from_id": f_id,
                "to_id": lv_node_id,
                "length_km": round(dist_lv, 4),
                "voltage_level": "LV"
            })
            line_count += 1

    # 3) Write output
    if output_format.lower() == "csv":
        fieldnames = ["line_id", "from_id", "to_id", "length_km", "voltage_level"]
        with open(output_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(lines_list)
        print(f"[create_mv_lv_lines] Created lines CSV: {output_path}")
    elif output_format.lower() == "json":
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(lines_list, f, indent=2)
        print(f"[create_mv_lv_lines] Created lines JSON: {output_path}")
    else:
        raise ValueError("output_format must be 'csv' or 'json'.")

    return lines_list

if __name__ == "__main__":
    """
    Example usage:
      Suppose we have 2 feeders, we define their lat/lon here.
      We'll create a substation at (40.10, -3.50).
      We'll generate an MV line from substation to each feeder.
      Then for each feeder, we create 2 LV branches as a demonstration.
    """
    feeder_list = [
        {"feeder_id": "Feeder1", "lat": 40.105, "lon": -3.505},
        {"feeder_id": "Feeder2", "lat": 40.110, "lon": -3.520}
    ]
    lines = create_mv_lv_lines(
        substation_id="MainSubstation",
        substation_lat=40.1000,
        substation_lon=-3.5000,
        feeder_nodes=feeder_list,
        lv_branches_per_feeder=2,  # purely for demonstration
        output_format="csv",
        output_path="lines_demo.csv"
    )
    print(f"Created {len(lines)} lines in lines_demo.csv.")

------------------------------------------------------------

File: generate_time_series_loads.py
============================================================
"""
generate_time_series_loads.py

Generates a wide-format CSV of time-series loads for multiple buildings
and multiple "Energy" categories (including new "cooling" and "battery_charge").

Output format (wide CSV):

| building_id | Energy            | 00:00:00 | 00:15:00 | 00:30:00 | ...
| B0001       | heating           |    0     |   0      |   11     | ...
| B0001       | cooling           |   12     |  10      |   15     | ...
| B0001       | facility          |   121    |  121     |   0      | ...
| B0001       | generation        | ...      | ...      |   ...    | ...
| B0001       | battery_charge    | ...      | ...      |   ...    | ...
| B0001       | total_electricity |   ...    | ...      |   ...    | ...
| B0002       | heating           | ...
...

We optionally use 'buildings_info' to check if a building has_solar or has_battery.
If so, we can produce non-zero generation or battery_charge. Otherwise zero.

Usage:
  python generate_time_series_loads.py
  -> By default, it will create 'time_series_loads.csv' for some sample building IDs.
"""

import csv
import random
from datetime import datetime, timedelta

def generate_time_stamps(
    start_time_str="2025-01-01 00:00:00",
    end_time_str="2025-01-01 06:00:00",
    step_minutes=15
):
    """
    Generates a list of time-step labels (HH:MM:SS) from 'start_time_str' to
    'end_time_str' in increments of 'step_minutes'. The end_time is exclusive.
    """
    fmt = "%Y-%m-%d %H:%M:%S"
    start_dt = datetime.strptime(start_time_str, fmt)
    end_dt = datetime.strptime(end_time_str, fmt)

    time_stamps = []
    current = start_dt
    while current < end_dt:
        time_str = current.strftime("%H:%M:%S")  # e.g. "00:00:00"
        time_stamps.append(time_str)
        current += timedelta(minutes=step_minutes)

    return time_stamps

def generate_random_profile(num_steps, category, has_solar=False, has_battery=False):
    """
    Returns a list (length=num_steps) of random values appropriate for the category.
    We incorporate logic for heating, cooling, facility, generation, battery_charge, etc.
    If has_solar=False, generation stays 0. If has_battery=False, battery stays 0.
    """
    values = []

    if category == "heating":
        # e.g. ~0..30 kW
        for _ in range(num_steps):
            val = round(random.uniform(0, 30), 1)
            values.append(val)

    elif category == "cooling":
        # e.g. ~0..25 kW
        for _ in range(num_steps):
            val = round(random.uniform(0, 25), 1)
            values.append(val)

    elif category == "facility":
        # e.g. ~50..150 kW
        for _ in range(num_steps):
            val = round(random.uniform(50, 150), 1)
            values.append(val)

    elif category == "generation":
        # if building has solar, produce some non-zero generation
        if has_solar:
            for _ in range(num_steps):
                val = round(random.uniform(0, 10), 1)
                values.append(val)
        else:
            values = [0.0]*num_steps

    elif category == "battery_charge":
        # if building has battery, produce random +/- flows
        if has_battery:
            for _ in range(num_steps):
                # e.g. + means charging, - means discharging
                val = round(random.uniform(-5, 5), 1)
                values.append(val)
        else:
            values = [0.0]*num_steps

    elif category == "storage":
        # legacy example category, e.g. 0..20 kW
        for _ in range(num_steps):
            val = round(random.uniform(0, 20), 1)
            values.append(val)

    elif category == "total_electricity":
        # If you want an actual sum of the above, do it outside. 
        # For now, just zero or dummy data.
        values = [0.0]*num_steps

    else:
        # fallback
        values = [0.0]*num_steps

    return values

def generate_time_series_loads(
    building_ids,
    categories=("heating","cooling","facility","generation","battery_charge","total_electricity"),
    start_time="2025-01-01 00:00:00",
    end_time="2025-01-01 06:00:00",
    step_minutes=15,
    output_csv="time_series_loads.csv",
    buildings_info=None
):
    """
    Creates a CSV with columns: [building_id, Energy, <time_1>, <time_2>, ...].
    Each building has one row per 'Energy' category.

    If 'buildings_info' is provided as a dict:
        buildings_info[bldg_id] = {
            "has_solar": bool,
            "has_battery": bool,
            ...
        }
    then we can produce generation or battery load only for those that have solar/battery.
    Otherwise, we default to random or 0 as needed.
    """
    print(f"[generate_time_series_loads] Generating time-series loads for {len(building_ids)} buildings.")

    # 1) Create time stamps
    time_stamps = generate_time_stamps(start_time, end_time, step_minutes)
    num_steps = len(time_stamps)

    # 2) Header
    header = ["building_id","Energy"] + time_stamps

    # 3) Prepare rows
    rows = []
    for b_id in building_ids:
        # If we have building-level info:
        has_solar = False
        has_battery = False
        if buildings_info and b_id in buildings_info:
            has_solar = bool(buildings_info[b_id].get("has_solar", False))
            has_battery = bool(buildings_info[b_id].get("has_battery", False))

        for cat in categories:
            # Generate random profile
            cat_values = generate_random_profile(num_steps, cat, has_solar, has_battery)
            row = [b_id, cat] + cat_values
            rows.append(row)

    # 4) Write CSV
    with open(output_csv, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        writer.writerows(rows)

    print(f"[generate_time_series_loads] Created '{output_csv}' with {len(rows)} rows.")

if __name__ == "__main__":
    # Example usage if run standalone:
    test_building_ids = ["B0001","B0002","B0003"]
    # Suppose B0001 has solar, B0002 has battery, B0003 has neither
    bldg_info_example = {
        "B0001": {"has_solar": True,  "has_battery": False},
        "B0002": {"has_solar": False, "has_battery": True},
        "B0003": {"has_solar": False, "has_battery": False}
    }

    generate_time_series_loads(
        building_ids=test_building_ids,
        categories=["heating","cooling","facility","generation","battery_charge","total_electricity"],
        start_time="2025-01-01 00:00:00",
        end_time="2025-01-01 06:00:00",
        step_minutes=15,
        output_csv="time_series_loads.csv",
        buildings_info=bldg_info_example
    )

------------------------------------------------------------

File: assign_buildings.py
============================================================
"""
assign_buildings.py

Assigns each building to the nearest LV line by geospatial proximity.
We need:
  - A buildings file (CSV or JSON) with columns "building_id", "lat", "lon", ...
  - A lines file (CSV or JSON) with "line_id", "from_id", "to_id", "voltage_level"...
  - A node_locations dictionary that gives lat/lon for each node_id (substation, feeder, etc.),
    so we can reconstruct line segments.

We output a CSV (or JSON) that lists each building's assigned line and the distance.
"""

import csv
import json
import math

def load_buildings(buildings_path):
    """
    Loads building data from a CSV or JSON, returns list of dicts:
      [ { "building_id": ..., "lat": ..., "lon": ..., ...}, ...]
    """
    if buildings_path.lower().endswith(".csv"):
        with open(buildings_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            return list(reader)
    else:
        # assume JSON
        with open(buildings_path, "r", encoding="utf-8") as f:
            return json.load(f)

def load_lines(lines_path):
    """
    Loads line data from a CSV or JSON, returns list of dicts:
      [ { "line_id": ..., "from_id": ..., "to_id": ..., "voltage_level": ..., ...}, ...]
    """
    if lines_path.lower().endswith(".csv"):
        with open(lines_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            return list(reader)
    else:
        # assume JSON
        with open(lines_path, "r", encoding="utf-8") as f:
            return json.load(f)

def distance_lat_lon(lat1, lon1, lat2, lon2):
    """
    Returns approximate distance in km (haversine).
    """
    R = 6371.0
    lat1_r = math.radians(float(lat1))
    lon1_r = math.radians(float(lon1))
    lat2_r = math.radians(float(lat2))
    lon2_r = math.radians(float(lon2))

    dlat = lat2_r - lat1_r
    dlon = lon2_r - lon1_r

    a = (math.sin(dlat/2)**2
         + math.cos(lat1_r)*math.cos(lat2_r)*math.sin(dlon/2)**2)
    c = 2*math.atan2(math.sqrt(a), math.sqrt(1-a))
    return R*c

def point_to_line_distance_km(px, py, x1, y1, x2, y2):
    """
    Computes the shortest distance from point P(px, py) to
    the line segment (x1, y1) -> (x2, y2), in km.
    1) Convert lat/lon to approximate local XY if the region is small,
       OR do an iterative approach with haversine for partial steps.
    2) For simplicity, we can do a rough "flat" approach if lat/lon bounding box is small.

    Below is a naive "flat earth" approach for short distances:
      - Convert lat/lon to approximate meters using a reference lat.
      - Do standard 2D point-line-segment distance.
      - Return result in km.

    If your region is large, consider a proper geodesic library or a simpler approach:
      - compare distance(P->A), distance(P->B), or find closest fraction along line.

    We'll do a simple approach that is acceptable for small bounding boxes.
    """

    # 1) Convert lat/lon to a rough meter grid
    # Choose a reference latitude for scale
    ref_lat = (y1 + y2) / 2
    # approx degrees to meters
    m_per_deg_lat = 111132.954
    m_per_deg_lon = 111132.954 * math.cos(math.radians(ref_lat))

    def latlon_to_xy(lat, lon):
        x = (lon - (-180)) * m_per_deg_lon  # naive shift, or do a local shift
        y = (lat - 0) * m_per_deg_lat
        return x, y

    # But let's do a simpler shift so we don't blow up the numbers
    # We'll shift relative to (ref_lat, mid of x1,x2).
    mid_lon = (x1 + x2)/2
    lat_shift = ref_lat
    lon_shift = mid_lon

    def local_xy(lat, lon):
        delta_lat = lat - lat_shift
        delta_lon = lon - lon_shift
        x_m = delta_lon * m_per_deg_lon
        y_m = delta_lat * m_per_deg_lat
        return x_m, y_m

    # Convert the line endpoints and point
    X1, Y1 = local_xy(y1, x1)
    X2, Y2 = local_xy(y2, x2)
    PX, PY = local_xy(py, px)

    # 2) Compute standard 2D point-line distance
    # Param of projection
    dx = X2 - X1
    dy = Y2 - Y1
    if dx == 0 and dy == 0:
        # from->to are same point
        dist_m = math.dist((PX, PY), (X1, Y1))
    else:
        t = ((PX - X1)*dx + (PY - Y1)*dy) / (dx*dx + dy*dy)
        if t < 0:
            # closest to segment start
            dist_m = math.dist((PX, PY), (X1, Y1))
        elif t > 1:
            # closest to segment end
            dist_m = math.dist((PX, PY), (X2, Y2))
        else:
            # projection in the middle
            projX = X1 + t*dx
            projY = Y1 + t*dy
            dist_m = math.dist((PX, PY), (projX, projY))

    dist_km = dist_m / 1000.0
    return dist_km

def assign_buildings_to_lines(buildings, lines, node_locations, only_lv=True):
    """
    :param buildings: list of dicts with {building_id, lat, lon, ...}
    :param lines: list of dicts with {line_id, from_id, to_id, voltage_level, ...}
    :param node_locations: dict { node_id: (lat, lon) }, for from_id/to_id
    :param only_lv: if True, we only consider lines where voltage_level == "LV"
    :return: list of assignments: [ { "building_id":..., "line_id":..., "distance_km":... }, ...]
    """
    assignments = []
    for b in buildings:
        b_id = b["building_id"]
        latB = float(b["lat"])
        lonB = float(b["lon"])

        best_line = None
        best_dist = 1e9

        for ln in lines:
            if only_lv and ln.get("voltage_level") != "LV":
                continue

            # get from_id lat/lon
            f_id = ln["from_id"]
            t_id = ln["to_id"]
            if f_id not in node_locations or t_id not in node_locations:
                # cannot compute geometry if we lack node coords
                continue

            lat1, lon1 = node_locations[f_id]
            lat2, lon2 = node_locations[t_id]

            # compute dist
            dist_km = point_to_line_distance_km(lonB, latB, lon1, lat1, lon2, lat2)
            # note the param order:
            #  point_to_line_distance_km(px, py, x1, y1, x2, y2)
            #  we used px=lonB, py=latB, x1=lon1, y1=lat1, etc.
            #  because we wrote it that way above.

            if dist_km < best_dist:
                best_dist = dist_km
                best_line = ln["line_id"]

        if best_line:
            assignments.append({
                "building_id": b_id,
                "line_id": best_line,
                "distance_km": round(best_dist, 5)
            })
        else:
            # no line found? Possibly store a negative or None
            assignments.append({
                "building_id": b_id,
                "line_id": None,
                "distance_km": None
            })

    return assignments

def write_assignments_csv(assignments, output_path="building_assignments.csv"):
    """
    Writes the assignment list to a CSV.
    """
    fieldnames = ["building_id", "line_id", "distance_km"]
    with open(output_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(assignments)

    print(f"[assign_buildings_to_lines] Wrote {len(assignments)} assignments to {output_path}")

if __name__ == "__main__":
    """
    Example usage:
      1) We have 'buildings_demo.csv' (with building_id, lat, lon).
      2) We have 'lines_demo.csv' (with line_id, from_id, to_id, voltage_level).
      3) We have a node_locations dict for "MainSubstation", "Feeder1", "Feeder2", 
         or any LV branch nodes used by lines.
    """
    # 1) Load building file
    buildings_file = "buildings_demo.csv"  # must contain lat/lon columns
    buildings_data = load_buildings(buildings_file)

    # 2) Load line file
    lines_file = "lines_demo.csv"
    lines_data = load_lines(lines_file)

    # 3) Suppose we keep node coords in a dictionary or from a file
    #    This must match "from_id"/"to_id" in lines_demo.csv
    node_locations = {
        "MainSubstation": (40.1000, -3.5000),
        "Feeder1": (40.1050, -3.5050),
        "Feeder2": (40.1100, -3.5200),
        "Feeder1_LVbranch_1": (40.1060, -3.5060),
        "Feeder1_LVbranch_2": (40.1070, -3.5070),
        "Feeder2_LVbranch_1": (40.1110, -3.5210),
        "Feeder2_LVbranch_2": (40.1120, -3.5220)
        # etc.
    }

    # 4) Assign buildings to nearest LV line
    assignments_list = assign_buildings_to_lines(buildings_data, lines_data, node_locations, only_lv=True)
    # 5) Write results
    write_assignments_csv(assignments_list, "building_assignments.csv")

------------------------------------------------------------

File: ascii_generator.py
============================================================
"""
ascii_generator.py

Generates a simple ASCII schematic diagram (tree-like) from the 'model' dict
produced by build_network_model.py.

Assumptions:
- We have exactly one "HV_Station" node feeding "MV_Node_0".
- From MV_Node_0, there are several feeders (MV_Node_i),
  each with a transformer to an LV_Node_i,
  which then links to building nodes.

If your model is more complex, you'll need to adapt the logic accordingly.
"""

def generate_ascii_diagram(model):
    """
    Returns a multi-line ASCII string representing the hierarchical layout:
      HV_Station -> MV_Node_0 -> feeders -> T(MV->LV) -> LV_Node -> buildings
    """
    # A helper function to get a node dict by ID
    def get_node_by_id(n_id):
        return next((n for n in model["nodes"] if n["id"] == n_id), None)

    # A helper to find lines from a given from_node
    def get_lines_from(src_id):
        return [l for l in model["lines"] if l["from_node"] == src_id]

    # A helper to find links from a given from_node (for buildings, etc.)
    def get_links_from(src_id):
        return [lk for lk in model["links"] if lk["from_node"] == src_id]

    lines = []

    # 1) Find the HV_Station node
    hv_node = None
    for n in model["nodes"]:
        if n["name"] == "HV_Station":
            hv_node = n
            break

    if hv_node is None:
        return "No HV_Station found in the model!"

    hv_str = f"{hv_node['name']} (id={hv_node['id']})"
    lines.append(hv_str)

    # 2) Find the line from HV_Station to MV_Node_0 (the main MV bus)
    hv_to_mv_line = None
    for l in model["lines"]:
        if l["from_node"] == hv_node["id"]:
            hv_to_mv_line = l
            break

    if hv_to_mv_line is None:
        lines.append("  |-> [No HV->MV connection found!]")
        return "\n".join(lines)

    mv_main = get_node_by_id(hv_to_mv_line["to_node"])
    mv_main_str = f"  |-> {mv_main['name']} (id={mv_main['id']}) [Main MV bus]"
    lines.append(mv_main_str)

    # 3) From MV_Node_0, we look for lines that feed MV_Node_i
    #    i.e. from_node = mv_main["id"], plus the MV->LV transformer line
    feeders = []
    for ln in model["lines"]:
        if ln["from_node"] == mv_main["id"] and "Feeder" in ln["name"]:
            feeders.append(ln)

    # Sort feeders by name or ID (for consistent ordering)
    feeders.sort(key=lambda x: x["name"])

    # 4) For each feeder line, get the MV_Node, then the transformer line -> LV_Node, etc.
    for feeder_line in feeders:
        mv_node_id = feeder_line["to_node"]
        mv_node = get_node_by_id(mv_node_id)
        line_str = f"      {feeder_line['name']}: {mv_node['name']} (id={mv_node_id})"
        lines.append(line_str)

        # Now find the transformer line from that MV_Node to an LV_Node
        tx_line = None
        for l in model["lines"]:
            if l["from_node"] == mv_node_id and "MVtoLV" in l["name"]:
                tx_line = l
                break
        
        if tx_line:
            tx_str = f"          {tx_line['name']} (id={tx_line['id']}) [Transformer]"
            lines.append(tx_str)

            lv_node = get_node_by_id(tx_line["to_node"])
            lv_str = f"            -> {lv_node['name']} (id={lv_node['id']}) [LV bus]"
            lines.append(lv_str)

            # Now find building links from the LV node
            bldg_links = get_links_from(lv_node["id"])
            # Sort them by name or ID for consistent ordering
            bldg_links.sort(key=lambda x: x["name"])

            for bl in bldg_links:
                bldg = get_node_by_id(bl["to_node"])
                b_str = f"                {bldg['name']} (id={bldg['id']}) [Building]"
                lines.append(b_str)
        else:
            lines.append("         [No MV->LV transformer found for this feeder!]")

    return "\n".join(lines)

------------------------------------------------------------

File: asym_output.json
============================================================
{
  "version": "1.0",
  "type": "asym_output",
  "data": {
    "node": [
      {
        "id": 1,
        "u_pu": [
          0.93,
          0.951,
          1.032
        ],
        "p": [
          -215924.8,
          145922.0,
          94029.4
        ]
      },
      {
        "id": 2,
        "u_pu": [
          1.008,
          1.016,
          0.945
        ]
      },
      {
        "id": 3,
        "u_pu": [
          1.009,
          0.989,
          0.945
        ],
        "p": [
          -260759.9,
          -63642.1,
          300843.8
        ]
      },
      {
        "id": 4,
        "u_pu": [
          1.021,
          0.991,
          0.95
        ],
        "p": [
          351827.4,
          471877.9,
          -282927.5
        ]
      },
      {
        "id": 5,
        "u_pu": [
          0.967,
          0.929,
          0.93
        ]
      },
      {
        "id": 6,
        "u_pu": [
          1.01,
          1.047,
          0.905
        ]
      },
      {
        "id": 7,
        "u_pu": [
          1.049,
          1.006,
          0.991
        ]
      },
      {
        "id": 14,
        "u_pu": [
          0.935,
          1.024,
          0.919
        ]
      },
      {
        "id": 16,
        "u_pu": [
          1.009,
          0.945,
          1.008
        ],
        "p": [
          -498851.5,
          -152158.9,
          -256336.1
        ]
      },
      {
        "id": 18,
        "u_pu": [
          1.039,
          0.977,
          0.923
        ],
        "p": [
          327712.1,
          649871.9,
          527488.1
        ]
      },
      {
        "id": 20,
        "u_pu": [
          0.974,
          0.956,
          0.946
        ],
        "p": [
          90556.6,
          521036.1,
          398613.7
        ]
      },
      {
        "id": 22,
        "u_pu": [
          1.035,
          0.967,
          0.972
        ],
        "p": [
          347981.7,
          561764.2,
          -164895.6
        ]
      }
    ],
    "line": [
      {
        "id": 8,
        "i_from": [
          112.079,
          42.014,
          16.263
        ]
      },
      {
        "id": 9,
        "i_from": [
          145.241,
          25.409,
          12.579
        ]
      },
      {
        "id": 10,
        "i_from": [
          104.619,
          71.556,
          131.15
        ]
      },
      {
        "id": 11,
        "i_from": [
          142.337,
          63.646,
          112.726
        ]
      },
      {
        "id": 12,
        "i_from": [
          116.633,
          98.847,
          47.944
        ]
      },
      {
        "id": 13,
        "i_from": [
          82.04,
          99.773,
          127.807
        ]
      }
    ],
    "shunt": []
  }
}
------------------------------------------------------------

File: json_generator.py
============================================================
"""
json_generator.py

Converts the 'model' (dict with nodes, lines, links, sources, loads, shunts)
into the requested JSON structure:

{
  "version": "1.0",
  "type": "input",
  "data": {
    "node": [...],
    "line": [...],
    "link": [...],
    "source": [...],
    "sym_load": [...],
    "asym_load": [...],
    "shunt": [...]
  }
}
"""

def generate_json_data(model):
    """
    Transforms the internal model into the required JSON format.
    Returns a Python dict that can be dumped via json.dumps().
    """
    output = {
        "version": "1.0",
        "type": "input",
        "data": {
            "node": [],
            "line": [],
            "link": [],
            "source": [],
            "sym_load": [],
            "asym_load": [],
            "shunt": []
        }
    }

    # 1) Map model["nodes"] -> "node"
    for nd in model["nodes"]:
        # Example node format:
        # { "id": 1, "u_rated": 10000, "extra": "First Node" }
        output["data"]["node"].append({
            "id": nd["id"],
            "u_rated": nd["u_rated"],
            "extra": nd["name"]  # store the node's "name" in 'extra'
        })

    # 2) Map model["lines"] -> "line"
    #    lines have r1, x1, i_n, from_node, to_node
    for ln in model["lines"]:
        # Example line format:
        # {
        #   "id": 4, "from_node": 1, "to_node": 2,
        #   "r1": 10.0, "x1": 0.0, "i_n": 1000
        # }
        output["data"]["line"].append({
            "id": ln["id"],
            "from_node": ln["from_node"],
            "to_node": ln["to_node"],
            "r1": ln.get("r1", 0.0),
            "x1": ln.get("x1", 0.0),
            "i_n": ln.get("i_n", 9999)
        })

    # 3) Map model["links"] -> "link"
    #    links are simpler, no R/X. Example format:
    #    { "id": 5, "from_node": 2, "to_node": 3 }
    for lk in model["links"]:
        output["data"]["link"].append({
            "id": lk["id"],
            "from_node": lk["from_node"],
            "to_node": lk["to_node"]
        })

    # 4) Map model["sources"] -> "source"
    #    Format from example:
    #    { "id": 6, "node": 1, "status": 1, "u_ref": 1.05 }
    for s in model["sources"]:
        output["data"]["source"].append({
            "id": s["id"],
            "node": s["node"],
            "status": s.get("status", 1),
            "u_ref": s.get("u_ref", 1.0)
        })

    # 5) Map model["loads"] -> "sym_load"
    #    Format from example:
    #    {
    #      "id": 7, "node": 3, "status": 1, "type": 1,
    #      "p_specified": 500000
    #    }
    #    We'll treat 'p_kW' as p_specified in watts.
    for ld in model["loads"]:
        p_watts = ld.get("p_kW", 0.0) * 1000.0
        output["data"]["sym_load"].append({
            "id": ld["id"],
            "node": ld["node"],
            "status": ld.get("status", 1),
            "type": ld.get("type", 1),
            "p_specified": p_watts
        })

    # 6) Map model["shunts"] -> "shunt"
    #    Format from example:
    #    { "id": 9, "node": 3, "status": 1, "g1": 0.015 }
    for sh in model["shunts"]:
        output["data"]["shunt"].append({
            "id": sh["id"],
            "node": sh["node"],
            "status": sh.get("status", 1),
            "g1": sh.get("g1", 0.0)
        })

    return output

------------------------------------------------------------

File: power_flow_solver.py
============================================================
"""
power_flow_solver.py

A dummy "power flow solver" that can:
  - read input JSON from a file and write sym_output.json / asym_output.json
  - or, solve in memory (no disk files) and return results as dict

In a real scenario, you'd parse the input more thoroughly,
run a real solver, and produce real results.
"""

import json
import random

def run_power_flow_sym(input_data, params_data=None):
    """
    Runs a dummy symmetrical power flow solution,
    returns a dict matching a 'sym_output.json' structure.

    input_data: a dict (the parsed JSON "input" with data.node, data.line, etc.)
    params_data: solver settings (ignored here, but in real solver you'd use them)
    """
    sym_output = {
        "version": "1.0",
        "type": "sym_output",
        "data": {
            "node": [],
            "line": [],
            "shunt": []
        }
    }

    # For each node in input_data["data"]["node"], create dummy results
    for nd in input_data["data"].get("node", []):
        node_id = nd["id"]
        # random p.u. voltage
        u_pu = round(random.uniform(0.90, 1.06), 3)
        # random injection
        p_dummy = round(random.uniform(-1e6, 2e6), 1)

        node_entry = {
            "id": node_id,
            "u_pu": u_pu
        }
        if random.random() < 0.7:
            node_entry["p"] = p_dummy

        sym_output["data"]["node"].append(node_entry)

    # lines
    for ln in input_data["data"].get("line", []):
        line_id = ln["id"]
        i_from = round(random.uniform(10, 200), 3)
        line_entry = {
            "id": line_id,
            "i_from": i_from
        }
        sym_output["data"]["line"].append(line_entry)

    # shunts
    for sh in input_data["data"].get("shunt", []):
        shunt_id = sh["id"]
        i_shunt = round(random.uniform(1, 50), 3)
        p_shunt = round(i_shunt * 14600, 1)
        shunt_entry = {
            "id": shunt_id,
            "i": i_shunt,
            "p": p_shunt
        }
        sym_output["data"]["shunt"].append(shunt_entry)

    return sym_output


def run_power_flow_asym(input_data, params_data=None):
    """
    Runs a dummy unbalanced (three-phase) power flow.
    Returns a dict shaped like 'asym_output.json'.

    We'll store 3-phase arrays for voltages, power, etc.
    """
    asym_output = {
        "version": "1.0",
        "type": "asym_output",
        "data": {
            "node": [],
            "line": [],
            "shunt": []
        }
    }

    for nd in input_data["data"].get("node", []):
        node_id = nd["id"]
        u_phases = [round(random.uniform(0.90, 1.05), 3) for _ in range(3)]
        p_phases = [round(random.uniform(-500000, 700000), 1) for _ in range(3)]

        node_entry = {
            "id": node_id,
            "u_pu": u_phases
        }
        if random.random() < 0.8:
            node_entry["p"] = p_phases

        asym_output["data"]["node"].append(node_entry)

    for ln in input_data["data"].get("line", []):
        line_id = ln["id"]
        i_phases = [round(random.uniform(5, 150), 3) for _ in range(3)]
        line_entry = {
            "id": line_id,
            "i_from": i_phases
        }
        asym_output["data"]["line"].append(line_entry)

    for sh in input_data["data"].get("shunt", []):
        shunt_id = sh["id"]
        i_shunt_phases = [round(random.uniform(1, 30), 3) for _ in range(3)]
        p_shunt_phases = [round(i * 18000, 1) for i in i_shunt_phases]
        shunt_entry = {
            "id": shunt_id,
            "i": i_shunt_phases,
            "p": p_shunt_phases
        }
        asym_output["data"]["shunt"].append(shunt_entry)

    return asym_output


def solve_power_flow_in_memory(input_dict, params_data=None):
    """
    Runs the power flow solver *in memory* (no files). 
    Returns:
      {
        "sym": {... sym_output ...},
        "asym": {... asym_output ...}
      }

    :param input_dict: the 'input.json' structure as a Python dict
    :param params_data: optional solver parameters
    """
    sym_results = run_power_flow_sym(input_dict, params_data)
    asym_results = run_power_flow_asym(input_dict, params_data)
    return {"sym": sym_results, "asym": asym_results}


def solve_power_flow(input_json_path, params_json_path=None,
                     sym_out_path="sym_output.json",
                     asym_out_path="asym_output.json"):
    """
    High-level function:
      1) Reads the input JSON (network model or input data) from file
      2) (Optional) reads solver params
      3) Runs symmetrical and asymmetrical PF
      4) Writes the results to sym_out_path, asym_out_path
    """
    # 1) Read input JSON
    with open(input_json_path, "r") as f:
        input_data = json.load(f)

    # 2) If we have solver params
    params_data = None
    if params_json_path:
        with open(params_json_path, "r") as f:
            params_data = json.load(f)

    # 3) Run symmetrical PF
    sym_results = run_power_flow_sym(input_data, params_data)

    # 4) Run asymmetrical PF
    asym_results = run_power_flow_asym(input_data, params_data)

    # 5) Write results
    with open(sym_out_path, "w") as f_sym:
        json.dump(sym_results, f_sym, indent=2)

    with open(asym_out_path, "w") as f_asym:
        json.dump(asym_results, f_asym, indent=2)

    print(f"Symmetrical results written to: {sym_out_path}")
    print(f"Asymmetrical results written to: {asym_out_path}")


# Example usage
if __name__ == "__main__":
    # If you run: python power_flow_solver.py
    # We'll do a quick demonstration of the in-memory function
    dummy_input = {
        "version": "1.0",
        "type": "input",
        "data": {
            "node": [{"id": 1}, {"id": 2}],
            "line": [{"id": 3, "from_node": 1, "to_node": 2}],
            "shunt": []
        }
    }

    results = solve_power_flow_in_memory(dummy_input)
    print("In-memory PF results:\n", results)

    # Or the file-based approach:
    # solve_power_flow("network_model.json", params_json_path=None)

------------------------------------------------------------

File: export_as_geojson.py
============================================================
"""
export_as_geojson.py

Creates GeoJSON files from:
  1) buildings_demo.csv  -> buildings_demo.geojson
  2) lines_demo.csv      -> lines_demo.geojson
  3) building_assignments.csv -> assignments_demo.geojson

We assume:
  - buildings_demo.csv has columns [building_id, lat, lon, building_type, peak_load_kW, ...]
  - lines_demo.csv has columns [line_id, from_id, to_id, length_km, voltage_level, ...]
  - building_assignments.csv has columns [building_id, line_id, distance_km, ...]
  - node_locations: a dict { node_id: (lat, lon) } for from_id/to_id references in lines_demo.csv

Dependencies: None beyond Python stdlib. We'll produce standard GeoJSON FeatureCollections.
"""

import csv
import json
import math
import os

def load_csv_as_list_of_dict(csv_path):
    with open(csv_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        return list(reader)

def export_buildings_geojson(buildings_csv, output_geojson="buildings_demo.geojson"):
    """
    Reads buildings CSV and creates a GeoJSON FeatureCollection of Points.
    Each Feature has geometry = Point(longitude, latitude)
    and properties for building_id, building_type, peak_load_kW, etc.
    """
    if not os.path.exists(buildings_csv):
        print(f"[export_buildings_geojson] File not found: {buildings_csv}")
        return

    buildings = load_csv_as_list_of_dict(buildings_csv)

    features = []
    for b in buildings:
        b_id = b.get("building_id", "Unknown")
        lat = float(b.get("lat", 0))
        lon = float(b.get("lon", 0))

        # Build the GeoJSON feature
        feat = {
            "type": "Feature",
            "geometry": {
                "type": "Point",
                "coordinates": [lon, lat]  # GeoJSON => [longitude, latitude]
            },
            "properties": {
                "building_id": b_id,
                # Include other columns as well
                "building_type": b.get("building_type", ""),
                "peak_load_kW": b.get("peak_load_kW", "")
                # Add more if you have them
            }
        }
        features.append(feat)

    fc = {
        "type": "FeatureCollection",
        "features": features
    }

    with open(output_geojson, "w", encoding="utf-8") as f:
        json.dump(fc, f, indent=2)
    print(f"[export_buildings_geojson] Created '{output_geojson}' with {len(features)} features.")


def export_lines_geojson(lines_csv, node_locations, output_geojson="lines_demo.geojson"):
    """
    Converts lines_demo.csv to a GeoJSON FeatureCollection of LineStrings,
    using 'node_locations' to find lat/lon for from_id and to_id.

    lines_csv columns: [line_id, from_id, to_id, length_km, voltage_level, ...]
    node_locations: dict { "Feeder1": (lat, lon), ... }
    """
    if not os.path.exists(lines_csv):
        print(f"[export_lines_geojson] File not found: {lines_csv}")
        return

    lines = load_csv_as_list_of_dict(lines_csv)

    features = []
    missing_count = 0

    for ln in lines:
        line_id = ln.get("line_id", "Unknown")
        from_id = ln.get("from_id", "")
        to_id = ln.get("to_id", "")
        dist_km = ln.get("length_km", "")
        voltage_level = ln.get("voltage_level", "")

        # Get lat/lon for from_id and to_id
        if from_id not in node_locations or to_id not in node_locations:
            missing_count += 1
            continue

        lat1, lon1 = node_locations[from_id]
        lat2, lon2 = node_locations[to_id]

        feat = {
            "type": "Feature",
            "geometry": {
                "type": "LineString",
                "coordinates": [
                    [lon1, lat1],
                    [lon2, lat2]
                ]
            },
            "properties": {
                "line_id": line_id,
                "from_id": from_id,
                "to_id": to_id,
                "length_km": dist_km,
                "voltage_level": voltage_level
            }
        }
        features.append(feat)

    fc = {
        "type": "FeatureCollection",
        "features": features
    }

    with open(output_geojson, "w", encoding="utf-8") as f:
        json.dump(fc, f, indent=2)
    print(f"[export_lines_geojson] Created '{output_geojson}' with {len(features)} line features.")
    if missing_count > 0:
        print(f"[export_lines_geojson] WARNING: {missing_count} lines missing node_locations.")


def export_building_assignments_geojson(buildings_csv, assignments_csv,
                                        output_geojson="assignments_demo.geojson"):
    """
    Produces a FeatureCollection of building points, each with a property
    for the assigned line and distance. Essentially merges building info with assignment info.

    geometry = Point at building lat/lon
    properties = { building_id, assigned_line_id, distance_km, ... }

    Note: This does NOT create line geometry from building to assigned line; it's just a point
    with properties. If you want a line from building->line, you'd do a small linestring.
    """
    if not os.path.exists(buildings_csv):
        print(f"[export_building_assignments_geojson] File not found: {buildings_csv}")
        return
    if not os.path.exists(assignments_csv):
        print(f"[export_building_assignments_geojson] File not found: {assignments_csv}")
        return

    buildings = load_csv_as_list_of_dict(buildings_csv)
    assignments = load_csv_as_list_of_dict(assignments_csv)

    # Build a dict to quickly get lat/lon from building_id
    bldg_coords = {}
    bldg_extra = {}
    for b in buildings:
        b_id = b.get("building_id", "")
        lat = float(b.get("lat", 0))
        lon = float(b.get("lon", 0))
        bldg_coords[b_id] = (lat, lon)
        bldg_extra[b_id] = b  # store entire row if needed

    features = []
    missing_count = 0

    for asg in assignments:
        b_id = asg.get("building_id", "")
        line_id = asg.get("line_id", "")
        dist_km = asg.get("distance_km", "")

        if b_id not in bldg_coords:
            missing_count += 1
            continue

        latB, lonB = bldg_coords[b_id]

        # Build the point feature
        props = {
            "building_id": b_id,
            "assigned_line_id": line_id,
            "distance_km": dist_km
        }
        # Optionally, also merge building_type, peak_load_kW, etc.
        if b_id in bldg_extra:
            props["building_type"] = bldg_extra[b_id].get("building_type", "")
            props["peak_load_kW"] = bldg_extra[b_id].get("peak_load_kW", "")

        feat = {
            "type": "Feature",
            "geometry": {
                "type": "Point",
                "coordinates": [lonB, latB]
            },
            "properties": props
        }
        features.append(feat)

    fc = {
        "type": "FeatureCollection",
        "features": features
    }

    with open(output_geojson, "w", encoding="utf-8") as f:
        json.dump(fc, f, indent=2)
    print(f"[export_building_assignments_geojson] Created '{output_geojson}' with {len(features)} features.")
    if missing_count > 0:
        print(f"[export_building_assignments_geojson] WARNING: {missing_count} buildings not found in CSV.")


if __name__ == "__main__":
    """
    Example usage:
      1) We'll define a node_locations for lines (substation, feeders, etc.)
      2) We'll export lines_demo.csv -> lines_demo.geojson
      3) We'll export buildings_demo.csv -> buildings_demo.geojson
      4) We'll export building_assignments.csv -> assignments_demo.geojson
    """
    # 1) Suppose we have the same node_locations as in main.py
    node_locations = {
        "MainSubstation": (40.150, -3.550),
        "Feeder1": (40.160, -3.555),
        "Feeder1_LVbranch_1": (40.161, -3.556),
        "Feeder1_LVbranch_2": (40.162, -3.557),
        "Feeder2": (40.170, -3.560),
        "Feeder2_LVbranch_1": (40.171, -3.561),
        "Feeder2_LVbranch_2": (40.172, -3.562)
    }

    # 2) Export lines
    export_lines_geojson("lines_demo.csv", node_locations, "lines_demo.geojson")

    # 3) Export buildings
    export_buildings_geojson("buildings_demo.csv", "buildings_demo.geojson")

    # 4) Export assignments
    export_building_assignments_geojson("buildings_demo.csv", "building_assignments.csv",
                                        "assignments_demo.geojson")

------------------------------------------------------------

File: feeders_demo.csv
============================================================
Error reading file: [Errno 2] No such file or directory: 'feeders_demo.csv'

------------------------------------------------------------

File: building_assignments.csv
============================================================
building_id,line_id,distance_km
B0001,L0002,0.57507
B0002,L0002,5.39118
B0003,L0006,2.53358
B0004,L0002,3.69445
B0005,L0002,4.32198

------------------------------------------------------------

File: buildings_demo.csv
============================================================
building_id,lat,lon,peak_load_kW,ogc_fid,pand_id,label,gem_hoogte,gem_bouwlagen,b3_dak_type,b3_opp_dak_plat,b3_opp_dak_schuin,postcode,area,perimeter,height,bouwjaar,age_range,average_wwr,building_function,building_type,infiltration_rate_min,infiltration_rate_max,occupant_density_min,occupant_density_max,dhw_liters_per_person_day_min,dhw_liters_per_person_day_max,heating_day_setpoint_min,heating_day_setpoint_max,cooling_day_setpoint_min,cooling_day_setpoint_max,lighting_power_density_wm2_min,lighting_power_density_wm2_max,fan_power_w,hrv_efficiency,has_solar,solar_capacity_kWp,has_battery,battery_capacity_kWh,battery_power_kW
B0001,40.12244,-3.54838,90.7,1251962,1.46E+13,B,2,1,flat,44,53,4844CD,118,32,8.4,1977,1992 - 2005,0.28,residential,Two-and-a-half-story House,0.52,0.89,15,21,49,59,19.9,20.9,24.0,25.0,3.9,5.2,66,0.75,False,0.0,True,9.7,9.6
B0002,40.14574,-3.59897,105.7,5430356,1.19E+13,A,5,3,multiple horizontal,92,22,3155XZ,193,27,6.3,2000,1945 - 1964,0.31,residential,Two-and-a-half-story House,0.55,0.96,10,18,36,59,19.1,20.1,23.8,24.8,4.4,5.4,115,0.75,True,7.4,False,0.0,0.0
B0003,40.19978,-3.58718,129.2,9294079,1.62E+13,C,5,4,pitched,57,33,1522GH,102,33,11.7,1933,1992 - 2005,0.29,non_residential,Education Function,0.72,1.17,10,19,4,9,19.7,20.7,24.2,25.2,9.5,13.8,139,0.75,True,3.6,False,0.0,0.0
B0004,40.15733,-3.53719,60.6,8178658,1.71E+13,C,3,3,pitched,63,33,8888XZ,144,66,11.6,1958,1992 - 2005,0.23,residential,Apartment,0.58,0.86,12,19,44,62,19.8,20.8,24.3,25.3,3.9,5.7,90,0.75,True,5.5,False,0.0,0.0
B0005,40.15196,-3.57781,94.4,7633728,1.86E+13,C,8,4,pitched,24,45,6492CD,45,59,8.8,1978,1992 - 2005,0.11,non_residential,Office Function,0.96,1.04,10,13,5,9,19.8,20.8,24.4,25.4,9.3,14.4,115,0.75,False,0.0,True,25.3,4.0

------------------------------------------------------------

File: lines_demo.csv
============================================================
line_id,from_id,to_id,length_km,voltage_level
L0001,MainSubstation,Feeder1,3.065,MV
L0002,Feeder1,Feeder1_LVbranch_1,0.14,LV
L0003,MainSubstation,Feeder2,5.9296,MV
L0004,Feeder2,Feeder2_LVbranch_1,0.14,LV
L0005,MainSubstation,Feeder3,4.9991,MV
L0006,Feeder3,Feeder3_LVbranch_1,0.1399,LV

------------------------------------------------------------

File: time_series_loads.csv
============================================================
Error reading file: [Errno 2] No such file or directory: 'time_series_loads.csv'

------------------------------------------------------------

File: network_model.json
============================================================
{
  "version": "1.0",
  "type": "input",
  "data": {
    "node": [
      {
        "id": 1,
        "u_rated": 20000,
        "extra": "MainSubstation"
      },
      {
        "id": 2,
        "u_rated": 20000,
        "extra": "Feeder1"
      },
      {
        "id": 3,
        "u_rated": 20000,
        "extra": "Feeder1_LVbranch_1"
      },
      {
        "id": 4,
        "u_rated": 20000,
        "extra": "Feeder2"
      },
      {
        "id": 5,
        "u_rated": 20000,
        "extra": "Feeder2_LVbranch_1"
      },
      {
        "id": 6,
        "u_rated": 20000,
        "extra": "Feeder3"
      },
      {
        "id": 7,
        "u_rated": 20000,
        "extra": "Feeder3_LVbranch_1"
      },
      {
        "id": 14,
        "u_rated": 400,
        "extra": "B0001"
      },
      {
        "id": 16,
        "u_rated": 400,
        "extra": "B0002"
      },
      {
        "id": 18,
        "u_rated": 400,
        "extra": "B0003"
      },
      {
        "id": 20,
        "u_rated": 400,
        "extra": "B0004"
      },
      {
        "id": 22,
        "u_rated": 400,
        "extra": "B0005"
      }
    ],
    "line": [
      {
        "id": 8,
        "from_node": 1,
        "to_node": 2,
        "r1": 1.0,
        "x1": 5.0,
        "i_n": 300
      },
      {
        "id": 9,
        "from_node": 2,
        "to_node": 3,
        "r1": 0.5,
        "x1": 1.0,
        "i_n": 300
      },
      {
        "id": 10,
        "from_node": 1,
        "to_node": 4,
        "r1": 1.0,
        "x1": 5.0,
        "i_n": 300
      },
      {
        "id": 11,
        "from_node": 4,
        "to_node": 5,
        "r1": 0.5,
        "x1": 1.0,
        "i_n": 300
      },
      {
        "id": 12,
        "from_node": 1,
        "to_node": 6,
        "r1": 1.0,
        "x1": 5.0,
        "i_n": 300
      },
      {
        "id": 13,
        "from_node": 6,
        "to_node": 7,
        "r1": 0.5,
        "x1": 1.0,
        "i_n": 300
      }
    ],
    "link": [
      {
        "id": 24,
        "from_node": 14,
        "to_node": 3
      },
      {
        "id": 25,
        "from_node": 16,
        "to_node": 3
      },
      {
        "id": 26,
        "from_node": 18,
        "to_node": 7
      },
      {
        "id": 27,
        "from_node": 20,
        "to_node": 3
      },
      {
        "id": 28,
        "from_node": 22,
        "to_node": 3
      }
    ],
    "source": [
      {
        "id": 29,
        "node": 1,
        "status": 1,
        "u_ref": 1.0
      }
    ],
    "sym_load": [
      {
        "id": 15,
        "node": 14,
        "status": 1,
        "type": 1,
        "p_specified": 90700.0
      },
      {
        "id": 17,
        "node": 16,
        "status": 1,
        "type": 1,
        "p_specified": 105700.0
      },
      {
        "id": 19,
        "node": 18,
        "status": 1,
        "type": 1,
        "p_specified": 129199.99999999999
      },
      {
        "id": 21,
        "node": 20,
        "status": 1,
        "type": 1,
        "p_specified": 60600.0
      },
      {
        "id": 23,
        "node": 22,
        "status": 1,
        "type": 1,
        "p_specified": 94400.0
      }
    ],
    "asym_load": [],
    "shunt": []
  }
}
------------------------------------------------------------

File: sym_output.json
============================================================
{
  "version": "1.0",
  "type": "sym_output",
  "data": {
    "node": [
      {
        "id": 1,
        "u_pu": 1.048,
        "p": -496413.7
      },
      {
        "id": 2,
        "u_pu": 1.016,
        "p": 335958.7
      },
      {
        "id": 3,
        "u_pu": 0.98
      },
      {
        "id": 4,
        "u_pu": 0.945
      },
      {
        "id": 5,
        "u_pu": 0.966
      },
      {
        "id": 6,
        "u_pu": 1.028,
        "p": 86162.3
      },
      {
        "id": 7,
        "u_pu": 0.934,
        "p": 1693708.6
      },
      {
        "id": 14,
        "u_pu": 0.987,
        "p": 406639.5
      },
      {
        "id": 16,
        "u_pu": 1.038
      },
      {
        "id": 18,
        "u_pu": 1.005
      },
      {
        "id": 20,
        "u_pu": 0.996
      },
      {
        "id": 22,
        "u_pu": 0.914,
        "p": 186884.3
      }
    ],
    "line": [
      {
        "id": 8,
        "i_from": 195.255
      },
      {
        "id": 9,
        "i_from": 194.44
      },
      {
        "id": 10,
        "i_from": 73.383
      },
      {
        "id": 11,
        "i_from": 109.0
      },
      {
        "id": 12,
        "i_from": 189.532
      },
      {
        "id": 13,
        "i_from": 30.646
      }
    ],
    "shunt": []
  }
}
------------------------------------------------------------

